Sure, let's calculate the size of the vocabulary after removing stop words for both the training and test datasets. Here is the complete Python script to do this:

```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import nltk

# Download stopwords from nltk
nltk.download('stopwords')

# Load the training and test datasets
train_data = pd.read_csv('movie_review_train.csv')
test_data = pd.read_csv('movie_review_test.csv')

# Combine all reviews into a single list for both datasets
train_reviews = train_data['text'].tolist()
test_reviews = test_data['text'].tolist()

# Initialize CountVectorizer with stop words
vectorizer = CountVectorizer(stop_words=stopwords.words('english'))

# Fit the vectorizer on the training data
vectorizer.fit(train_reviews)

# Get the size of the vocabulary for the training set
train_vocab_size = len(vectorizer.vocabulary_)

# Fit the vectorizer on the test data
vectorizer.fit(test_reviews)

# Get the size of the vocabulary for the test set
test_vocab_size = len(vectorizer.vocabulary_)

print(f"The size of the vocabulary after removing stop words for the training set is: {train_vocab_size}")
print(f"The size of the vocabulary after removing stop words for the test set is: {test_vocab_size}")
```

Make sure you have the necessary libraries installed. You can install them using pip if you haven't already:

```bash
pip install pandas scikit-learn nltk
```

This script will output the size of the vocabulary after removing stop words for both the training and test datasets. Note that the actual size may vary slightly depending on the preprocessing steps and the specific stop words list used.
